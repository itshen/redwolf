"""
å¤šå¹³å°APIæœåŠ¡æ¨¡å—
æ”¯æŒé˜¿é‡Œäº‘ç™¾ç‚¼ã€OpenRouterã€Ollamaã€LMStudioç­‰å¹³å°
"""

import httpx
import json
import asyncio
from typing import Dict, List, Any, Optional, AsyncGenerator
from dataclasses import dataclass
from enum import Enum
import logging

# é…ç½®æ—¥å¿—
import os
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'

logging.basicConfig(level=logging.DEBUG if DEBUG_MODE else logging.INFO)
logger = logging.getLogger(__name__)

def debug_print(*args, **kwargs):
    """ç»Ÿä¸€çš„DEBUGè¾“å‡ºå‡½æ•°ï¼Œåªåœ¨DEBUG_MODEå¯ç”¨æ—¶è¾“å‡º"""
    if DEBUG_MODE:
        print(*args, **kwargs)

class PlatformType(Enum):
    """å¹³å°ç±»å‹æšä¸¾"""
    DASHSCOPE = "dashscope"  # é˜¿é‡Œäº‘ç™¾ç‚¼
    OPENROUTER = "openrouter"
    OLLAMA = "ollama"
    LMSTUDIO = "lmstudio"
    SILICONFLOW = "siliconflow"  # ç¡…åŸºæµåŠ¨
    OPENAI_COMPATIBLE = "openai_compatible"  # OpenAIå…¼å®¹

@dataclass
class PlatformConfig:
    """å¹³å°é…ç½®"""
    platform_type: PlatformType
    api_key: str = ""
    base_url: str = ""
    enabled: bool = True
    timeout: int = 30

@dataclass
class ModelInfo:
    """æ¨¡å‹ä¿¡æ¯"""
    id: str
    name: str
    platform: PlatformType
    enabled: bool = True
    description: str = ""

class PlatformClient:
    """å¹³å°å®¢æˆ·ç«¯åŸºç±»"""
    
    def __init__(self, config: PlatformConfig):
        self.config = config
        self.client = None
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨"""
        raise NotImplementedError
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """èŠå¤©è¡¥å…¨æ¥å£"""
        raise NotImplementedError
    
    async def test_connection(self) -> bool:
        """æµ‹è¯•è¿æ¥"""
        try:
            models = await self.get_models()
            return len(models) > 0
        except Exception as e:
            logger.error(f"Platform {self.config.platform_type} connection test failed: {e}")
            return False

class DashScopeClient(PlatformClient):
    """é˜¿é‡Œäº‘ç™¾ç‚¼å®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = "https://dashscope.aliyuncs.com"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–é€šä¹‰åƒé—®æ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [DashScope] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        
        if not self.config.api_key:
            logger.warning("âš ï¸ [DashScope] API Keyæœªé…ç½®ï¼Œè·³è¿‡è·å–æ¨¡å‹")
            return []
        
        try:
            logger.info(f"ğŸŒ [DashScope] è¯·æ±‚URL: {self.base_url}/compatible-mode/v1/models")
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/compatible-mode/v1/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                logger.info(f"ğŸ“¡ [DashScope] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [DashScope] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    # è§£ææ¨¡å‹åˆ—è¡¨
                    if "output" in data and "models" in data["output"]:
                        for model in data["output"]["models"]:
                            model_name = model.get("model_name", "")
                            model_info = ModelInfo(
                                id=model_name,
                                name=model_name,
                                platform=PlatformType.DASHSCOPE,
                                description=f"å®¹é‡: {model.get('base_capacity', 1)}"
                            )
                            models.append(model_info)
    
                    elif "data" in data:
                        # å…¼å®¹æ—§æ ¼å¼
                        for model in data["data"]:
                            model_info = ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("name", model.get("id", "")),
                                platform=PlatformType.DASHSCOPE,
                                description=model.get("description", "")
                            )
                            models.append(model_info)
    
                    else:
                        # å¦‚æœAPIè¿”å›æ ¼å¼ä¸åŒ¹é…ï¼Œæ·»åŠ ä¸€äº›é»˜è®¤çš„é€šä¹‰åƒé—®æ¨¡å‹
                        logger.info("âš ï¸ [DashScope] APIå“åº”æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
                        default_models = [
                            {"id": "qwen-plus", "name": "qwen-plus", "description": "é€šä¹‰åƒé—®å¢å¼ºç‰ˆ"},
                            {"id": "qwen-turbo", "name": "qwen-turbo", "description": "é€šä¹‰åƒé—®å¿«é€Ÿç‰ˆ"},
                            {"id": "qwen-max", "name": "qwen-max", "description": "é€šä¹‰åƒé—®æœ€å¼ºç‰ˆ"},
                            {"id": "qwen-coder", "name": "qwen-coder", "description": "ä¸“é—¨ç”¨äºä»£ç ç”Ÿæˆå’Œä¼˜åŒ–"},
                            {"id": "qwen3-coder-plus", "name": "qwen3-coder-plus", "description": "é€šä¹‰åƒé—®3ä»£ç å¢å¼ºç‰ˆ"},
                            {"id": "qwen2.5-coder-instruct", "name": "qwen2.5-coder-instruct", "description": "é€šä¹‰åƒé—®2.5ä»£ç æŒ‡ä»¤ç‰ˆ"},
                            {"id": "qwen2-72b-instruct", "name": "qwen2-72b-instruct", "description": "é€šä¹‰åƒé—®2 72BæŒ‡ä»¤ç‰ˆ"},
                        ]
                        
                        for model in default_models:
                            model_info = ModelInfo(
                                id=model["id"],
                                name=model["name"],
                                platform=PlatformType.DASHSCOPE,
                                description=model["description"]
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"âœ… [DashScope] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [DashScope] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [DashScope] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """é€šä¹‰åƒé—®èŠå¤©è¡¥å…¨"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/compatible-mode/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"DashScope chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OpenRouterClient(PlatformClient):
    """OpenRouterå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = "https://openrouter.ai/api/v1"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–OpenRouteræ¨¡å‹åˆ—è¡¨"""
        if not self.config.api_key:
            return []
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("name", model.get("id", "")),
                                platform=PlatformType.OPENROUTER,
                                description=model.get("description", "")
                            ))
                    
                    return models
                else:
                    logger.error(f"OpenRouter API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get OpenRouter models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenRouterèŠå¤©è¡¥å…¨"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    # ç›´æ¥ yield åŸå§‹è¡Œï¼Œè®©è½¬æ¢å™¨å¤„ç†æ ¼å¼
                                    yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"OpenRouter chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OllamaClient(PlatformClient):
    """Ollamaå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:11434"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–Ollamaæ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [Ollama] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        logger.info(f"ğŸŒ [Ollama] è¯·æ±‚URL: {self.base_url}/api/tags")
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/api/tags")
                
                logger.info(f"ğŸ“¡ [Ollama] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [Ollama] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    if "models" in data:
                        for model in data["models"]:
                            model_info = ModelInfo(
                                id=model.get("name", ""),
                                name=model.get("name", ""),
                                platform=PlatformType.OLLAMA,
                                description=f"Size: {model.get('size', 'Unknown')}"
                            )
                            models.append(model_info)
            
                    
                    logger.info(f"âœ… [Ollama] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [Ollama] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [Ollama] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = True,  # Ollamaé»˜è®¤ä½¿ç”¨æµå¼
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OllamaèŠå¤©è¡¥å…¨"""
        url = f"{self.base_url}/api/chat"
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    try:
                                        data = json.loads(line)
                                        # è½¬æ¢Ollamaæ ¼å¼åˆ°OpenAIæ ¼å¼
                                        openai_chunk = self._convert_ollama_to_openai(data)
                                        yield json.dumps(openai_chunk)
                                        
                                        if data.get("done", False):
                                            break
                                    except json.JSONDecodeError:
                                        continue
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    # éæµå¼æ¨¡å¼éœ€è¦æ‰‹åŠ¨æ”¶é›†æ‰€æœ‰å“åº”
                    full_response = ""
                    async with client.stream("POST", url, json=payload) as response:
                        async for line in response.aiter_lines():
                            if line.strip():
                                try:
                                    data = json.loads(line)
                                    if "message" in data and "content" in data["message"]:
                                        full_response += data["message"]["content"]
                                    if data.get("done", False):
                                        break
                                except json.JSONDecodeError:
                                    continue
                    
                    openai_response = {
                        "id": "chatcmpl-ollama",
                        "object": "chat.completion",
                        "created": int(asyncio.get_event_loop().time()),
                        "model": model,
                        "choices": [{
                            "index": 0,
                            "message": {
                                "role": "assistant",
                                "content": full_response
                            },
                            "finish_reason": "stop"
                        }]
                    }
                    yield json.dumps(openai_response)
                        
        except Exception as e:
            logger.error(f"Ollama chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})
    
    def _convert_ollama_to_openai(self, ollama_data: Dict[str, Any]) -> Dict[str, Any]:
        """å°†Ollamaå“åº”æ ¼å¼è½¬æ¢ä¸ºOpenAIæ ¼å¼"""
        content = ""
        if "message" in ollama_data and "content" in ollama_data["message"]:
            content = ollama_data["message"]["content"]
        
        return {
            "id": "chatcmpl-ollama",
            "object": "chat.completion.chunk",
            "created": int(asyncio.get_event_loop().time()),
            "model": ollama_data.get("model", "unknown"),
            "choices": [{
                "index": 0,
                "delta": {
                    "content": content
                } if content else {},
                "finish_reason": "stop" if ollama_data.get("done", False) else None
            }]
        }

class SiliconFlowClient(PlatformClient):
    """ç¡…åŸºæµåŠ¨å®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = "https://api.siliconflow.cn"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–ç¡…åŸºæµåŠ¨æ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [SiliconFlow] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        
        if not self.config.api_key:
            logger.warning("âš ï¸ [SiliconFlow] API Keyæœªé…ç½®ï¼Œè·³è¿‡è·å–æ¨¡å‹")
            return []
        
        try:
            logger.info(f"ğŸŒ [SiliconFlow] è¯·æ±‚URL: {self.base_url}/v1/models")
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    f"{self.base_url}/v1/models",
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                logger.info(f"ğŸ“¡ [SiliconFlow] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [SiliconFlow] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    # è§£ææ¨¡å‹åˆ—è¡¨
                    if "data" in data:
                        for model in data["data"]:
                            model_id = model.get("id", "")
                            model_name = model.get("name", model_id)
                            
                            model_info = ModelInfo(
                                id=model_id,
                                name=model_name,
                                platform=PlatformType.SILICONFLOW,
                                description=model.get("description", f"ç¡…åŸºæµåŠ¨æ¨¡å‹: {model_id}")
                            )
                            models.append(model_info)
                    else:
                        # å¦‚æœAPIè¿”å›æ ¼å¼ä¸åŒ¹é…ï¼Œæ·»åŠ ä¸€äº›é»˜è®¤çš„ç¡…åŸºæµåŠ¨æ¨¡å‹
                        logger.info("âš ï¸ [SiliconFlow] APIå“åº”æ ¼å¼ä¸åŒ¹é…ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹åˆ—è¡¨")
                        default_models = [
                            {"id": "Qwen/QwQ-32B", "name": "QwQ-32B", "description": "åƒé—®æ¨ç†æ¨¡å‹32Bç‰ˆæœ¬"},
                            {"id": "Qwen/Qwen2.5-72B-Instruct", "name": "Qwen2.5-72B-Instruct", "description": "åƒé—®2.5 72BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "Qwen/Qwen2.5-32B-Instruct", "name": "Qwen2.5-32B-Instruct", "description": "åƒé—®2.5 32BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "Qwen/Qwen2.5-14B-Instruct", "name": "Qwen2.5-14B-Instruct", "description": "åƒé—®2.5 14BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "Qwen/Qwen2.5-7B-Instruct", "name": "Qwen2.5-7B-Instruct", "description": "åƒé—®2.5 7BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "meta-llama/Llama-3.1-70B-Instruct", "name": "Llama-3.1-70B-Instruct", "description": "Llama 3.1 70BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "meta-llama/Llama-3.1-8B-Instruct", "name": "Llama-3.1-8B-Instruct", "description": "Llama 3.1 8BæŒ‡ä»¤ç‰ˆ"},
                            {"id": "deepseek-ai/DeepSeek-V2.5", "name": "DeepSeek-V2.5", "description": "æ·±åº¦æ±‚ç´¢V2.5æ¨¡å‹"},
                        ]
                        
                        for model in default_models:
                            model_info = ModelInfo(
                                id=model["id"],
                                name=model["name"],
                                platform=PlatformType.SILICONFLOW,
                                description=model["description"]
                            )
                            models.append(model_info)
                    
                    logger.info(f"âœ… [SiliconFlow] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [SiliconFlow] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [SiliconFlow] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """ç¡…åŸºæµåŠ¨èŠå¤©è¡¥å…¨"""
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"SiliconFlow chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class OpenAICompatibleClient(PlatformClient):
    """OpenAIå…¼å®¹å®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        # base_url å¿…é¡»ç”±ç”¨æˆ·é…ç½®ï¼Œæ²¡æœ‰é»˜è®¤å€¼
        self.base_url = config.base_url
        if not self.base_url:
            logger.warning("âš ï¸ [OpenAI Compatible] Base URLæœªé…ç½®")
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–OpenAIå…¼å®¹APIæ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸ” [OpenAI Compatible] å¼€å§‹è·å–æ¨¡å‹åˆ—è¡¨...")
        
        if not self.base_url:
            logger.warning("âš ï¸ [OpenAI Compatible] Base URLæœªé…ç½®ï¼Œè·³è¿‡è·å–æ¨¡å‹")
            return []
        
        if not self.config.api_key:
            logger.warning("âš ï¸ [OpenAI Compatible] API Keyæœªé…ç½®ï¼Œè·³è¿‡è·å–æ¨¡å‹")
            return []
        
        try:
            # ç¡®ä¿URLä»¥/ç»“å°¾
            base_url = self.base_url.rstrip('/')
            url = f"{base_url}/v1/models"
            
            logger.info(f"ğŸŒ [OpenAI Compatible] è¯·æ±‚URL: {url}")
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(
                    url,
                    headers={
                        "Authorization": f"Bearer {self.config.api_key}",
                        "Content-Type": "application/json"
                    }
                )
                
                logger.info(f"ğŸ“¡ [OpenAI Compatible] APIå“åº”çŠ¶æ€: {response.status_code}")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    logger.info(f"ğŸ“‹ [OpenAI Compatible] å“åº”æ•°æ®: {json.dumps(data, indent=2, ensure_ascii=False)}")
                    
                    # è§£ææ¨¡å‹åˆ—è¡¨
                    if "data" in data:
                        for model in data["data"]:
                            model_id = model.get("id", "")
                            model_name = model.get("name", model_id)
                            
                            model_info = ModelInfo(
                                id=model_id,
                                name=model_name,
                                platform=PlatformType.OPENAI_COMPATIBLE,
                                description=model.get("description", f"OpenAIå…¼å®¹æ¨¡å‹: {model_id}")
                            )
                            models.append(model_info)
                    else:
                        # å¦‚æœAPIè¿”å›æ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•ç›´æ¥ä½¿ç”¨å“åº”æ•°æ®
                        logger.info("âš ï¸ [OpenAI Compatible] APIå“åº”æ ¼å¼ä¸åŒ¹é…ï¼Œå°è¯•ç›´æ¥è§£æ")
                        if isinstance(data, list):
                            for model in data:
                                if isinstance(model, dict):
                                    model_id = model.get("id", str(model))
                                    model_info = ModelInfo(
                                        id=model_id,
                                        name=model.get("name", model_id),
                                        platform=PlatformType.OPENAI_COMPATIBLE,
                                        description=model.get("description", f"OpenAIå…¼å®¹æ¨¡å‹: {model_id}")
                                    )
                                    models.append(model_info)
                        else:
                            logger.warning("âš ï¸ [OpenAI Compatible] æ— æ³•è§£ææ¨¡å‹æ•°æ®ï¼Œè¯·æ£€æŸ¥APIå“åº”æ ¼å¼")
                    
                    logger.info(f"âœ… [OpenAI Compatible] æˆåŠŸè·å– {len(models)} ä¸ªæ¨¡å‹")
                    return models
                else:
                    logger.error(f"âŒ [OpenAI Compatible] APIé”™è¯¯: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"âŒ [OpenAI Compatible] è·å–æ¨¡å‹å¤±è´¥: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """OpenAIå…¼å®¹èŠå¤©è¡¥å…¨"""
        if not self.base_url:
            yield json.dumps({"error": "Base URL not configured"})
            return
        
        if not self.config.api_key:
            yield json.dumps({"error": "API key not configured"})
            return
        
        # ç¡®ä¿URLä»¥/ç»“å°¾
        base_url = self.base_url.rstrip('/')
        url = f"{base_url}/chat/completions"
        
        headers = {
            "Authorization": f"Bearer {self.config.api_key}",
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"OpenAI Compatible chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class LMStudioClient(PlatformClient):
    """LMStudioå®¢æˆ·ç«¯"""
    
    def __init__(self, config: PlatformConfig):
        super().__init__(config)
        self.base_url = config.base_url or "http://localhost:1234"
    
    async def get_models(self) -> List[ModelInfo]:
        """è·å–LMStudioæ¨¡å‹åˆ—è¡¨"""
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                response = await client.get(f"{self.base_url}/v1/models")
                
                if response.status_code == 200:
                    data = response.json()
                    models = []
                    
                    if "data" in data:
                        for model in data["data"]:
                            models.append(ModelInfo(
                                id=model.get("id", ""),
                                name=model.get("id", ""),
                                platform=PlatformType.LMSTUDIO,
                                description="LMStudio local model"
                            ))
                    
                    return models
                else:
                    logger.error(f"LMStudio API error: {response.status_code} - {response.text}")
                    return []
                    
        except Exception as e:
            logger.error(f"Failed to get LMStudio models: {e}")
            return []
    
    async def chat_completion(
        self, 
        model: str, 
        messages: List[Dict[str, Any]], 
        stream: bool = False,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """LMStudioèŠå¤©è¡¥å…¨"""
        url = f"{self.base_url}/v1/chat/completions"
        headers = {
            "Content-Type": "application/json"
        }
        
        payload = {
            "model": model,
            "messages": messages,
            "stream": stream,
            **kwargs
        }
        
        try:
            async with httpx.AsyncClient(timeout=self.config.timeout) as client:
                if stream:
                    async with client.stream(
                        "POST", url, headers=headers, json=payload
                    ) as response:
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    if line.startswith("data: "):
                                        data = line[6:]
                                        if data.strip() == "[DONE]":
                                            break
                                        yield data
                                    else:
                                        yield line
                        else:
                            error_msg = await response.aread()
                            yield json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                else:
                    response = await client.post(url, headers=headers, json=payload)
                    if response.status_code == 200:
                        yield response.text
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                        
        except Exception as e:
            logger.error(f"LMStudio chat completion error: {e}")
            yield json.dumps({"error": f"Request failed: {str(e)}"})

class PlatformManager:
    """å¹³å°ç®¡ç†å™¨"""
    
    def __init__(self):
        self.platforms: Dict[PlatformType, PlatformClient] = {}
    
    def add_platform(self, config: PlatformConfig):
        """æ·»åŠ å¹³å°"""
        if config.platform_type == PlatformType.DASHSCOPE:
            client = DashScopeClient(config)
        elif config.platform_type == PlatformType.OPENROUTER:
            client = OpenRouterClient(config)
        elif config.platform_type == PlatformType.OLLAMA:
            client = OllamaClient(config)
        elif config.platform_type == PlatformType.LMSTUDIO:
            client = LMStudioClient(config)
        elif config.platform_type == PlatformType.SILICONFLOW:
            client = SiliconFlowClient(config)
        elif config.platform_type == PlatformType.OPENAI_COMPATIBLE:
            client = OpenAICompatibleClient(config)
        else:
            raise ValueError(f"Unsupported platform type: {config.platform_type}")
        
        self.platforms[config.platform_type] = client
    
    def get_platform(self, platform_type: PlatformType) -> Optional[PlatformClient]:
        """è·å–å¹³å°å®¢æˆ·ç«¯"""
        return self.platforms.get(platform_type)
    
    async def get_all_models(self) -> List[ModelInfo]:
        """è·å–æ‰€æœ‰å¹³å°çš„æ¨¡å‹åˆ—è¡¨"""
        logger.info("ğŸš€ [PlatformManager] å¼€å§‹è·å–æ‰€æœ‰å¹³å°æ¨¡å‹åˆ—è¡¨...")
        
        all_models = []
        for platform_type, platform in self.platforms.items():
            try:
                logger.info(f"ğŸ“ [PlatformManager] è°ƒç”¨ {platform_type.value} å¹³å°...")
                models = await platform.get_models()
                logger.info(f"ğŸ“¦ [PlatformManager] {platform_type.value} è¿”å› {len(models)} ä¸ªæ¨¡å‹")
                all_models.extend(models)
            except Exception as e:
                logger.error(f"âŒ [PlatformManager] {platform_type.value} å¹³å°è·å–æ¨¡å‹å¤±è´¥: {e}")
        
        logger.info(f"ğŸ¯ [PlatformManager] æ€»å…±è·å–åˆ° {len(all_models)} ä¸ªæ¨¡å‹")
        return all_models
    
    async def test_all_connections(self) -> Dict[PlatformType, bool]:
        """æµ‹è¯•æ‰€æœ‰å¹³å°è¿æ¥"""
        results = {}
        for platform_type, client in self.platforms.items():
            results[platform_type] = await client.test_connection()
        
        return results
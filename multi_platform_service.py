"""
å¤šå¹³å°APIæœåŠ¡
æ•´åˆæ‰€æœ‰ç»„ä»¶ï¼Œæä¾›ç»Ÿä¸€çš„APIæ¥å£
"""

import json
import asyncio
import logging
import httpx
from typing import Dict, List, Any, Optional, AsyncGenerator
from sqlalchemy.orm import Session
from fastapi import Response
from fastapi.responses import StreamingResponse

from platforms import PlatformManager, PlatformConfig, PlatformType
from routing_system import RoutingManager, RoutingMode
from format_converter import FormatConverter, StreamingConverter
from database import (
    PlatformConfig as DBPlatformConfig, 
    ModelConfig, 
    SystemConfig,
    get_db
)

# é…ç½®æ—¥å¿—
import os
DEBUG_MODE = os.getenv('DEBUG_MODE', 'false').lower() == 'true'

logging.basicConfig(level=logging.DEBUG if DEBUG_MODE else logging.INFO)
logger = logging.getLogger(__name__)

def debug_print(*args, **kwargs):
    """ç»Ÿä¸€çš„DEBUGè¾“å‡ºå‡½æ•°ï¼Œåªåœ¨DEBUG_MODEå¯ç”¨æ—¶è¾“å‡º"""
    if DEBUG_MODE:
        print(*args, **kwargs)

class MultiPlatformService:
    """å¤šå¹³å°APIæœåŠ¡"""
    
    def __init__(self):
        self.platform_manager = PlatformManager()
        self.routing_manager = RoutingManager(self.platform_manager)
        self.format_converter = FormatConverter()
        self.streaming_converter = None  # æ¯æ¬¡è¯·æ±‚æ—¶åˆ›å»ºæ–°çš„å®ä¾‹
        self.initialized = False
    
    async def initialize(self, db: Session):
        """åˆå§‹åŒ–æœåŠ¡ï¼ŒåŠ è½½é…ç½®"""
        logger.info("ğŸš€ [MultiPlatformService] å¼€å§‹åˆå§‹åŒ–å¤šå¹³å°æœåŠ¡...")
        
        try:
            logger.info("ğŸ“‹ [MultiPlatformService] åŠ è½½å¹³å°é…ç½®...")
            await self._load_platform_configs(db)
            
            logger.info("ğŸ§­ [MultiPlatformService] åŠ è½½è·¯ç”±é…ç½®...")
            self.routing_manager.load_config(db)
            
            self.initialized = True
            logger.info("âœ… [MultiPlatformService] å¤šå¹³å°æœåŠ¡åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            logger.error(f"âŒ [MultiPlatformService] åˆå§‹åŒ–å¤±è´¥: {e}")
            self.initialized = False
    
    async def _load_platform_configs(self, db: Session):
        """åŠ è½½å¹³å°é…ç½®"""
        logger.info("ğŸ” [MultiPlatformService] æŸ¥è¯¢æ•°æ®åº“ä¸­çš„å¹³å°é…ç½®...")
        
        platform_configs = db.query(DBPlatformConfig).filter(
            DBPlatformConfig.enabled == True
        ).all()
        
        logger.info(f"ğŸ“Š [MultiPlatformService] æ‰¾åˆ° {len(platform_configs)} ä¸ªå¯ç”¨çš„å¹³å°é…ç½®")
        
        for db_config in platform_configs:
            try:
                logger.info(f"âš™ï¸ [MultiPlatformService] åŠ è½½ {db_config.platform_type} å¹³å°é…ç½®...")
                
                platform_type = PlatformType(db_config.platform_type)
                config = PlatformConfig(
                    platform_type=platform_type,
                    api_key=db_config.api_key or "",
                    base_url=db_config.base_url or "",
                    enabled=db_config.enabled,
                    timeout=db_config.timeout
                )
                
                self.platform_manager.add_platform(config)
                logger.info(f"âœ… [MultiPlatformService] {platform_type.value} å¹³å°é…ç½®åŠ è½½æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"âŒ [MultiPlatformService] åŠ è½½ {db_config.platform_type} å¹³å°é…ç½®å¤±è´¥: {e}")
    
    async def handle_request(
        self, 
        messages: List[Dict[str, Any]], 
        model: str = "",
        stream: bool = False,
        db: Session = None,
        original_request: Dict[str, Any] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        # ä¿å­˜è·¯ç”±ä¿¡æ¯ä¾›å¤–éƒ¨è®¿é—®
        self.last_routing_result = None
        # ä¿å­˜HOOKå¤„ç†æ•°æ®ä¾›å¤–éƒ¨è®¿é—®
        self.processed_prompt = None
        self.processed_headers = None
        self.model_raw_headers = None
        self.model_raw_response = None
        """å¤„ç†èŠå¤©è¯·æ±‚"""
        if not self.initialized:
            if db:
                await self.initialize(db)
            else:
                yield json.dumps({"error": "Service not initialized"})
                return
        
        # 1. åˆ¤æ–­è·¯ç”±æ¨¡å¼
        routing_result = await self.routing_manager.route_request(messages)
        self.last_routing_result = routing_result
        
        if not routing_result.success:
            if routing_result.error_message == "Use original Claude Code API":
                # ä½¿ç”¨åŸæœ‰çš„Claude Code APIé€»è¾‘
                yield json.dumps({"error": "Should use original Claude Code API"})
                return
            else:
                yield json.dumps({"error": routing_result.error_message})
                return
        
        # 2. è½¬æ¢æ¶ˆæ¯æ ¼å¼
        openai_messages = self.format_converter.claude_to_openai(messages)
        
        # å¤„ç†systemå‚æ•°ï¼šå¦‚æœæœ‰systemå­—æ®µï¼Œæ·»åŠ ä¸ºsystem message
        extracted_tools = None
        if "system" in kwargs and kwargs["system"]:
            system_content = kwargs["system"]
            
            # å¦‚æœsystemæ˜¯æ•°ç»„æ ¼å¼ï¼ˆClaudeæ ¼å¼ï¼‰ï¼Œæå–æ–‡æœ¬å†…å®¹å’Œå·¥å…·ä¿¡æ¯
            if isinstance(system_content, list):
                text_parts = []
                for item in system_content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_parts.append(item.get("text", ""))
                system_content = "\n".join(text_parts)
            
            if system_content:  # ç¡®ä¿æœ‰å†…å®¹
                system_message = {
                    "role": "system",
                    "content": system_content
                }
                # å°†system messageæ’å…¥åˆ°æ¶ˆæ¯åˆ—è¡¨å¼€å¤´
                openai_messages.insert(0, system_message)
                debug_print(f"[DEBUG] æ·»åŠ system message: {system_content[:100]}...")
        
        # æš‚æ—¶ä¿å­˜è½¬æ¢åçš„æ¶ˆæ¯ï¼Œç¨åä¼šç”¨å®Œæ•´çš„payloadè¦†ç›–
        self.processed_prompt = None
        
        # 3. è·å–ç›®æ ‡å¹³å°å®¢æˆ·ç«¯
        client = self.platform_manager.get_platform(routing_result.platform_type)
        if not client:
            yield json.dumps({"error": f"Platform {routing_result.platform_type} not available"})
            return
        
        # 4. åˆ›å»ºæµå¼è½¬æ¢å™¨ï¼ˆæ¯æ¬¡è¯·æ±‚éƒ½æ˜¯æ–°çš„å®ä¾‹ï¼‰
        if stream:
            debug_print(f"[DEBUG] MultiPlatformService: åˆ›å»ºæµå¼è½¬æ¢å™¨, original_model={model}, target_model={routing_result.model_id}")
            self.streaming_converter = StreamingConverter(original_model=model)
            # ä¼°ç®—è¾“å…¥tokenæ•°é‡
            estimated_input_tokens = self._estimate_input_tokens(openai_messages)
            self.streaming_converter.total_input_tokens = estimated_input_tokens
            debug_print(f"[DEBUG] MultiPlatformService: ä¼°ç®—è¾“å…¥tokens: {estimated_input_tokens}")
        
        # 5. å¤„ç† tools å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œè½¬æ¢ä¸º system promptï¼‰
        tools_processed = False
        tools_to_process = None
        
        # ä¼˜å…ˆæ£€æŸ¥ç‹¬ç«‹çš„toolså‚æ•°
        if "tools" in kwargs and kwargs["tools"]:
            tools_to_process = kwargs["tools"]
            debug_print(f"[DEBUG] æ£€æµ‹åˆ°ç‹¬ç«‹çš„toolså‚æ•°")
        # å¦‚æœæ²¡æœ‰ç‹¬ç«‹çš„toolså‚æ•°ï¼Œæ£€æŸ¥åŸå§‹è¯·æ±‚ä¸­æ˜¯å¦æœ‰toolså­—æ®µ
        elif original_request and "tools" in original_request and original_request["tools"]:
            tools_to_process = original_request["tools"]
            debug_print(f"[DEBUG] ä»åŸå§‹è¯·æ±‚ä¸­æ£€æµ‹åˆ°toolså‚æ•°ï¼ŒåŒ…å« {len(tools_to_process)} ä¸ªå·¥å…·")
        
        if tools_to_process:
            openai_messages = self._convert_tools_to_system_prompt(openai_messages, tools_to_process)
            debug_print(f"[DEBUG] å·²å°†toolsè½¬æ¢ä¸ºsystem prompt")
            tools_processed = True
        
        # 6. è¿‡æ»¤å’Œè½¬æ¢ä¸æ”¯æŒçš„å‚æ•°
        filtered_kwargs = self._filter_unsupported_params(kwargs, routing_result.platform_type)
        
        # ç§»é™¤systemå‚æ•°ï¼ˆå› ä¸ºå·²ç»è½¬æ¢ä¸ºsystem messageäº†ï¼‰
        if "system" in filtered_kwargs:
            filtered_kwargs.pop("system")
            debug_print(f"[DEBUG] ç§»é™¤systemå‚æ•°ï¼ˆå·²è½¬æ¢ä¸ºsystem messageï¼‰")
        
        # é’ˆå¯¹ä¸åŒå¹³å°è°ƒæ•´å‚æ•°é™åˆ¶
        filtered_kwargs = self._adjust_platform_limits(filtered_kwargs, routing_result.platform_type)
        
        # å¦‚æœå·²ç»å¤„ç†äº† toolsï¼Œç§»é™¤ç›¸å…³å‚æ•°é¿å…å†²çª
        if tools_processed:
            filtered_kwargs.pop("tools", None)
            filtered_kwargs.pop("tool_choice", None)
            debug_print(f"[DEBUG] ç§»é™¤äº† tools å’Œ tool_choice å‚æ•°ï¼Œé¿å…ä¸ system prompt å†²çª")
        
        debug_print(f"[DEBUG] å‘é€åˆ°{routing_result.platform_type.value}çš„å‚æ•°: {filtered_kwargs.keys()}")
        
        # 7. è°ƒç”¨ç›®æ ‡API - ç›´æ¥ä½¿ç”¨httpxè·å–å®Œæ•´å“åº”ä¿¡æ¯
        try:
            # æ„å»ºAPIè¯·æ±‚å‚æ•°
            api_url = self._get_api_url(client, routing_result.platform_type)
            headers = self._get_api_headers(client, routing_result.platform_type)
            
            payload = {
                "model": routing_result.model_id,
                "messages": openai_messages,
                "stream": stream,
                **filtered_kwargs
            }
            
            # ä¿å­˜çœŸæ­£å‘ç»™è¿œç«¯å¤§æ¨¡å‹çš„å®Œæ•´è¯·æ±‚å†…å®¹ï¼ˆHOOKå¤„ç†åçš„åŸæ ·ï¼‰
            self.processed_prompt = json.dumps(payload, ensure_ascii=False, indent=2)
            self.processed_headers = json.dumps(headers, ensure_ascii=False, indent=2)
            
            debug_print(f"[DEBUG] è°ƒç”¨API: {api_url}")
            # åªæ˜¾ç¤ºå…³é”®ä¿¡æ¯ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
            debug_payload = {
                "model": payload.get("model"),
                "stream": payload.get("stream"),
                "messages_count": len(payload.get("messages", [])),
                "first_message_role": payload.get("messages", [{}])[0].get("role") if payload.get("messages") else None,
                "last_message_role": payload.get("messages", [{}])[-1].get("role") if payload.get("messages") else None,
                "other_params": [k for k in payload.keys() if k not in ["messages", "model", "stream"]]
            }
            debug_print(f"[DEBUG] è¯·æ±‚payloadæ¦‚è¦: {json.dumps(debug_payload, ensure_ascii=False, indent=2)}")
            
            async with httpx.AsyncClient(timeout=30.0) as http_client:
                if stream:
                    # æµå¼è¯·æ±‚
                    raw_response_chunks = []
                    async with http_client.stream("POST", api_url, headers=headers, json=payload) as response:
                        # ä¿å­˜å“åº”å¤´
                        self.model_raw_headers = json.dumps(dict(response.headers), ensure_ascii=False, indent=2)
                        debug_print(f"[DEBUG] è·å–åˆ°å“åº”å¤´: {response.status_code}")
                        
                        if response.status_code == 200:
                            async for line in response.aiter_lines():
                                if line.strip():
                                    raw_response_chunks.append(line)
                                    
                                    # è½¬æ¢å“åº”æ ¼å¼
                                    platform_type_str = routing_result.platform_type.value
                                    if platform_type_str == "dashscope":
                                        converter_type = "qwen"
                                    elif platform_type_str == "openrouter":
                                        converter_type = "openrouter"
                                    elif platform_type_str == "ollama":
                                        converter_type = "ollama"
                                    elif platform_type_str == "lmstudio":
                                        converter_type = "lmstudio"
                                    else:
                                        converter_type = "openai"
                                    
                                    converted_chunk = await self.streaming_converter.convert_stream(line, converter_type)
                                    if converted_chunk:
                                        yield converted_chunk
                        else:
                            error_msg = await response.aread()
                            error_data = json.dumps({"error": f"API error: {response.status_code} - {error_msg.decode()}"})
                            raw_response_chunks.append(error_data)
                            yield error_data
                    
                    # ä¿å­˜æµå¼å“åº”æ•°æ®
                    self.model_raw_response = "\n".join(raw_response_chunks)
                    
                else:
                    # éæµå¼è¯·æ±‚
                    response = await http_client.post(api_url, headers=headers, json=payload)
                    
                    # ä¿å­˜å“åº”å¤´å’Œå“åº”ä½“
                    self.model_raw_headers = json.dumps(dict(response.headers), ensure_ascii=False, indent=2)
                    self.model_raw_response = response.text
                    
                    debug_print(f"[DEBUG] éæµå¼å“åº”: {response.status_code}, å“åº”é•¿åº¦: {len(response.text)}")
                    
                    if response.status_code == 200:
                        # è½¬æ¢å“åº”æ ¼å¼
                        converted_response = self.format_converter.openai_to_claude(response.text, is_stream=False, original_model=model)
                        yield converted_response
                    else:
                        yield json.dumps({"error": f"API error: {response.status_code} - {response.text}"})
                    
        except Exception as e:
            logger.error(f"Failed to call platform API: {e}")
            # å¦‚æœæ˜¯æµå¼è¯·æ±‚ä¸”æœ‰è½¬æ¢å™¨ï¼Œéœ€è¦å‘é€é”™è¯¯æ ¼å¼
            if stream and self.streaming_converter:
                error_event = {
                    "type": "error",
                    "error": {
                        "type": "api_error",
                        "message": f"API call failed: {str(e)}"
                    }
                }
                yield f"event: error\ndata: {json.dumps(error_event, ensure_ascii=False)}\n\n"
            else:
                yield json.dumps({"error": f"API call failed: {str(e)}"})
    
    async def get_available_models(self, db: Session) -> List[Dict[str, Any]]:
        """è·å–æ‰€æœ‰å¯ç”¨æ¨¡å‹"""
        logger.info("ğŸ“‹ [MultiPlatformService] è·å–å¯ç”¨æ¨¡å‹åˆ—è¡¨...")
        
        if not self.initialized:
            logger.info("ğŸ”„ [MultiPlatformService] æœåŠ¡æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            await self.initialize(db)
        
        # ä¼˜å…ˆä»æ•°æ®åº“è·å–æ¨¡å‹åˆ—è¡¨
        logger.info("ğŸ’¾ [MultiPlatformService] ä¼˜å…ˆä»æ•°æ®åº“è·å–æ¨¡å‹åˆ—è¡¨...")
        db_models = db.query(ModelConfig).filter(ModelConfig.enabled == True).all()
        
        if db_models:
            logger.info(f"ğŸ“‹ [MultiPlatformService] ä»æ•°æ®åº“è·å–åˆ° {len(db_models)} ä¸ªæ¨¡å‹")
            result_models = []
            for model in db_models:
                # é¿å…é‡å¤æ·»åŠ å¹³å°å‰ç¼€
                model_id = model.model_id
                if not model_id.startswith(f"{model.platform_type}:"):
                    model_id = f"{model.platform_type}:{model.model_id}"
                
                model_dict = {
                    "id": model_id,
                    "name": model.model_name,
                    "platform": model.platform_type,
                    "description": model.description or "",
                    "enabled": model.enabled
                }
                result_models.append(model_dict)

            
            logger.info(f"âœ… [MultiPlatformService] ä»æ•°æ®åº“è¿”å› {len(result_models)} ä¸ªå¯ç”¨æ¨¡å‹")
            return result_models
        else:
            # å¦‚æœæ•°æ®åº“ä¸ºç©ºï¼Œåˆ™ä»APIè·å–å¹¶ä¿å­˜
            logger.info("ğŸ“ [MultiPlatformService] æ•°æ®åº“ä¸ºç©ºï¼Œä»APIè·å–æ¨¡å‹...")
            all_models = await self.platform_manager.get_all_models()
            
            if all_models:
                # ä¿å­˜åˆ°æ•°æ®åº“
                await self._save_models_to_db(db, all_models)
                
                # é‡æ–°ä»æ•°æ®åº“è¯»å–
                return await self.get_available_models(db)
            else:
                logger.warning("âš ï¸ [MultiPlatformService] æœªè·å–åˆ°ä»»ä½•æ¨¡å‹")
                return []
    
    async def test_platform_connections(self, db: Session) -> Dict[str, bool]:
        """æµ‹è¯•æ‰€æœ‰å¹³å°è¿æ¥"""
        if not self.initialized:
            await self.initialize(db)
        
        results = await self.platform_manager.test_all_connections()
        
        return {
            platform_type.value: status 
            for platform_type, status in results.items()
        }
    
    async def refresh_models(self, db: Session, platform_type: str = None):
        """åˆ·æ–°æ¨¡å‹åˆ—è¡¨å¹¶ä¿å­˜åˆ°æ•°æ®åº“"""
        logger.info("ğŸ”„ [MultiPlatformService] å¼€å§‹åˆ·æ–°æ¨¡å‹åˆ—è¡¨...")
        
        if not self.initialized:
            logger.info("ğŸ”„ [MultiPlatformService] æœåŠ¡æœªåˆå§‹åŒ–ï¼Œå¼€å§‹åˆå§‹åŒ–...")
            await self.initialize(db)
        
        if platform_type:
            # åˆ·æ–°ç‰¹å®šå¹³å°çš„æ¨¡å‹
            logger.info(f"ğŸ¯ [MultiPlatformService] åˆ·æ–°ç‰¹å®šå¹³å°: {platform_type}")
            try:
                platform_enum = PlatformType(platform_type)
                client = self.platform_manager.get_platform(platform_enum)
                if client:
                    logger.info(f"ğŸ“ [MultiPlatformService] è·å– {platform_type} å¹³å°æ¨¡å‹...")
                    models = await client.get_models()
                    logger.info(f"ğŸ’¾ [MultiPlatformService] ä¿å­˜ {len(models)} ä¸ªæ¨¡å‹åˆ°æ•°æ®åº“...")
                    await self._save_models_to_db(db, models)
                else:
                    logger.warning(f"âš ï¸ [MultiPlatformService] æœªæ‰¾åˆ° {platform_type} å¹³å°å®¢æˆ·ç«¯")
            except ValueError:
                logger.error(f"âŒ [MultiPlatformService] æ— æ•ˆçš„å¹³å°ç±»å‹: {platform_type}")
        else:
            # åˆ·æ–°æ‰€æœ‰å¹³å°çš„æ¨¡å‹
            logger.info("ğŸŒ [MultiPlatformService] åˆ·æ–°æ‰€æœ‰å¹³å°çš„æ¨¡å‹...")
            all_models = await self.platform_manager.get_all_models()
            logger.info(f"ğŸ’¾ [MultiPlatformService] ä¿å­˜ {len(all_models)} ä¸ªæ¨¡å‹åˆ°æ•°æ®åº“...")
            await self._save_models_to_db(db, all_models)
    
    async def _save_models_to_db(self, db: Session, models: List):
        """ä¿å­˜æ¨¡å‹åˆ°æ•°æ®åº“"""
        logger.info(f"ğŸ’¾ [MultiPlatformService] å¼€å§‹ä¿å­˜ {len(models)} ä¸ªæ¨¡å‹åˆ°æ•°æ®åº“...")
        
        saved_count = 0
        updated_count = 0
        
        for model in models:
            try:
                # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
                existing = db.query(ModelConfig).filter(
                    ModelConfig.platform_type == model.platform.value,
                    ModelConfig.model_id == model.id
                ).first()
                
                if existing:
                    # æ›´æ–°ç°æœ‰æ¨¡å‹
                    existing.model_name = model.name
                    existing.description = model.description
                    updated_count += 1
        
                else:
                    # åˆ›å»ºæ–°æ¨¡å‹
                    new_model = ModelConfig(
                        platform_type=model.platform.value,
                        model_id=model.id,
                        model_name=model.name,
                        description=model.description,
                        enabled=model.enabled
                    )
                    db.add(new_model)
                    saved_count += 1

                    
            except Exception as e:
                logger.error(f"âŒ [MultiPlatformService] ä¿å­˜æ¨¡å‹å¤±è´¥ {model.platform.value}:{model.id}: {e}")
        
        try:
            db.commit()
            logger.info(f"âœ… [MultiPlatformService] æ•°æ®åº“ä¿å­˜å®Œæˆ: æ–°å¢ {saved_count} ä¸ªï¼Œæ›´æ–° {updated_count} ä¸ªæ¨¡å‹")
        except Exception as e:
            logger.error(f"âŒ [MultiPlatformService] æ•°æ®åº“æäº¤å¤±è´¥: {e}")
            db.rollback()
    
    def get_current_routing_mode(self) -> str:
        """è·å–å½“å‰è·¯ç”±æ¨¡å¼"""
        return self.routing_manager.get_current_mode().value
    
    def get_platform_info(self, platform_type) -> dict:
        """è·å–å¹³å°ä¿¡æ¯"""
        client = self.platform_manager.get_platform(platform_type)
        if client:
            # ä¼˜å…ˆä½¿ç”¨å®¢æˆ·ç«¯çš„base_urlå±æ€§ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é…ç½®ä¸­çš„base_url
            base_url = getattr(client, 'base_url', None) or (client.config.base_url if hasattr(client, 'config') else None)
            return {
                "base_url": base_url or "unknown",
                "platform_name": platform_type.value
            }
        return {
            "base_url": "unknown",
            "platform_name": platform_type.value if platform_type else "unknown"
        }
    
    def _filter_unsupported_params(self, kwargs: Dict[str, Any], platform_type) -> Dict[str, Any]:
        """è¿‡æ»¤å¹³å°ä¸æ”¯æŒçš„å‚æ•°"""
        # åªè¿‡æ»¤ä¼šå¯¼è‡´APIè°ƒç”¨å¤±è´¥çš„å…³é”®å‚æ•°
        # å¯¹äºOpenRouterï¼Œä¿ç•™OpenAIæ ¼å¼çš„æ‰€æœ‰å‚æ•°ï¼Œåªå¤„ç†ç‰¹æ®Šå†²çªæƒ…å†µ
        unsupported_params = {
            "dashscope": [
                "tools", "tool_choice", "metadata", 
                "anthropic-version", "anthropic-beta", "anthropic-dangerous-direct-browser-access"
            ],
            "openrouter": [
                # åªè¿‡æ»¤Anthropicç‰¹æœ‰çš„å¤´éƒ¨å‚æ•°ï¼Œä¿ç•™OpenAIæ ¼å¼çš„å‚æ•°
                "anthropic-version", "anthropic-beta", "anthropic-dangerous-direct-browser-access"
            ],
            "ollama": ["tools", "tool_choice", "metadata", "anthropic-version", "anthropic-beta"],
            "lmstudio": ["tools", "tool_choice", "metadata", "anthropic-version", "anthropic-beta"]
        }
        
        platform_name = platform_type.value
        filtered = {}
        removed_params = []
        
        for key, value in kwargs.items():
            # é€šç”¨è¿‡æ»¤è§„åˆ™
            if platform_name in unsupported_params and key in unsupported_params[platform_name]:
                removed_params.append(key)
                continue
            
            # OpenRouter ç‰¹æ®Šè§„åˆ™ï¼šå¦‚æœæ²¡æœ‰ toolsï¼Œå°±ä¸èƒ½æœ‰ tool_choice
            if platform_name == "openrouter" and key == "tool_choice":
                if "tools" not in kwargs or not kwargs["tools"]:
                    removed_params.append(key)
                    debug_print(f"[DEBUG] OpenRouter: ç”±äºæ²¡æœ‰toolså‚æ•°ï¼Œç§»é™¤tool_choice")
                    continue
            
            filtered[key] = value
        
        if removed_params:
            debug_print(f"[DEBUG] è¿‡æ»¤æ‰{platform_name}ä¸æ”¯æŒçš„å‚æ•°: {removed_params}")
        
        return filtered
    
    def _adjust_platform_limits(self, kwargs: Dict[str, Any], platform_type) -> Dict[str, Any]:
        """æ ¹æ®å¹³å°é™åˆ¶è°ƒæ•´å‚æ•°"""
        adjusted = kwargs.copy()
        platform_name = platform_type.value
        
        # DashScopeå¹³å°é™åˆ¶
        if platform_name == "dashscope":
            # max_tokensé™åˆ¶: 1-8192
            if "max_tokens" in adjusted:
                original_value = adjusted["max_tokens"]
                if original_value > 8192:
                    adjusted["max_tokens"] = 8192
                    debug_print(f"[DEBUG] DashScope: max_tokensä»{original_value}è°ƒæ•´ä¸º8192")
                elif original_value < 1:
                    adjusted["max_tokens"] = 1
                    debug_print(f"[DEBUG] DashScope: max_tokensä»{original_value}è°ƒæ•´ä¸º1")
        
        # å…¶ä»–å¹³å°å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ é™åˆ¶é€»è¾‘
        # elif platform_name == "openrouter":
        #     # OpenRouterçš„é™åˆ¶
        #     pass
        
        return adjusted
    
    def _get_api_url(self, client, platform_type) -> str:
        """è·å–å¹³å°API URL"""
        platform_name = platform_type.value
        
        if platform_name == "dashscope":
            return "https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions"
        elif platform_name == "openrouter":
            return "https://openrouter.ai/api/v1/chat/completions"
        elif platform_name == "ollama":
            base_url = getattr(client, 'base_url', 'http://localhost:11434')
            return f"{base_url}/api/chat"
        elif platform_name == "lmstudio":
            base_url = getattr(client, 'base_url', 'http://localhost:1234')
            return f"{base_url}/v1/chat/completions"
        else:
            raise ValueError(f"Unsupported platform: {platform_name}")
    
    def _get_api_headers(self, client, platform_type) -> dict:
        """è·å–å¹³å°APIè¯·æ±‚å¤´"""
        platform_name = platform_type.value
        headers = {"Content-Type": "application/json"}
        
        if platform_name == "dashscope":
            headers["Authorization"] = f"Bearer {client.config.api_key}"
        elif platform_name == "openrouter":
            headers["Authorization"] = f"Bearer {client.config.api_key}"
        elif platform_name == "ollama":
            # Ollamaé€šå¸¸ä¸éœ€è¦è®¤è¯
            pass
        elif platform_name == "lmstudio":
            # LMStudioé€šå¸¸ä¸éœ€è¦è®¤è¯
            pass
        
        return headers
    
    def _convert_tools_to_system_prompt(self, messages: List[Dict[str, Any]], tools: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å°†toolså‚æ•°è½¬æ¢ä¸ºsystem promptï¼Œæ”¯æŒå®Œæ•´çš„Tool Useæµç¨‹"""
        if not tools:
            return messages
        
        # æ„å»ºè¯¦ç»†çš„toolsæè¿°ï¼ŒæŒ‡å¯¼æ¨¡å‹ä½¿ç”¨ <use_tool> æ ¼å¼
        tools_description = "\n\n=== Available Tools ===\n"
        tools_description += "You have access to the following tools. You MUST follow the exact XML format specified below.\n\n"
        
        for tool in tools:
            name = tool.get("name", "Unknown")
            description = tool.get("description", "No description")
            schema = tool.get("input_schema", {})
            
            tools_description += f"**{name}**\n"
            tools_description += f"Description: {description}\n"
            
            # æ·»åŠ å‚æ•°ä¿¡æ¯
            if "properties" in schema:
                tools_description += "Parameters:\n"
                for param_name, param_info in schema["properties"].items():
                    param_type = param_info.get("type", "unknown")
                    param_desc = param_info.get("description", "No description")
                    required = param_name in schema.get("required", [])
                    req_mark = " (required)" if required else " (optional)"
                    tools_description += f"  - {param_name} ({param_type}){req_mark}: {param_desc}\n"
            
            tools_description += "\n"
        
        # æ·»åŠ å·¥å…·ä½¿ç”¨æ ¼å¼è¯´æ˜ - æ›´ä¸¥æ ¼çš„çº¦æŸ
        tools_description += """**CRITICAL TOOL USAGE REQUIREMENTS:**

YOU MUST use tools in the EXACT format specified below. NO EXCEPTIONS.

**MANDATORY FORMAT:**
<use_tool>
<tool_name>exact_tool_name</tool_name>
<parameters>
{
  "parameter1": "value1",
  "parameter2": "value2"
}
</parameters>
</use_tool>

**STRICT RULES:**
1. NEVER use descriptive text like "UseTool: ToolName" or "Param: {...}"
2. ALWAYS use the <use_tool> XML tags exactly as shown
3. Tool names MUST match exactly what's listed above
4. Parameters MUST be valid JSON format
5. NO additional text between the XML tags
6. NO explanations inside the tool call

**CORRECT Example:**
<use_tool>
<tool_name>Bash</tool_name>
<parameters>
{
  "command": "ls -la",
  "description": "List files"
}
</parameters>
</use_tool>

**WRONG Examples (DO NOT USE):**
âŒ Tool: Bash
âŒ Param: {"command": "ls"}
âŒ Tool call: Bash with parameters...
âŒ Using tool Bash...

If you use ANY format other than the exact <use_tool> XML format, the tool call will FAIL.

**IMPORTANT REMINDERS:**
- Do NOT explain tool calls in natural language
- Do NOT use Chinese descriptive text like "Tool"
- Do NOT use any format other than <use_tool> XML tags
- The system can ONLY process the exact XML format shown above
- Multiple tools can be used by repeating the <use_tool> block
- You can only use one tool at a time

**COMPLIANCE CHECK:**
Before responding, verify that ALL tool calls use the exact format:
<use_tool><tool_name>NAME</tool_name><parameters>{JSON}</parameters></use_tool>

"""
        
        # æŸ¥æ‰¾systemæ¶ˆæ¯å¹¶é™„åŠ toolsæè¿°
        modified_messages = []
        system_found = False
        
        for message in messages:
            if message.get("role") == "system":
                # å°†toolsæè¿°é™„åŠ åˆ°ç°æœ‰systemæ¶ˆæ¯
                content = message.get("content", "")
                message["content"] = content + tools_description
                system_found = True
                debug_print(f"[DEBUG] å°†toolsæè¿°é™„åŠ åˆ°ç°æœ‰systemæ¶ˆæ¯")
            modified_messages.append(message)
        
        # å¦‚æœæ²¡æœ‰systemæ¶ˆæ¯ï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„
        if not system_found:
            system_message = {
                "role": "system",
                "content": tools_description
            }
            modified_messages.insert(0, system_message)
            debug_print(f"[DEBUG] åˆ›å»ºæ–°çš„systemæ¶ˆæ¯åŒ…å«toolsæè¿°")
        
        return modified_messages
    
    def _estimate_input_tokens(self, messages: List[Dict[str, Any]]) -> int:
        """ä¼°ç®—è¾“å…¥æ¶ˆæ¯çš„tokenæ•°é‡"""
        total_tokens = 0
        for message in messages:
            content = message.get("content", "")
            if isinstance(content, str):
                total_tokens += self._estimate_text_tokens(content)
            elif isinstance(content, list):
                for item in content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        total_tokens += self._estimate_text_tokens(item.get("text", ""))
        return total_tokens
    
    def _estimate_text_tokens(self, text: str) -> int:
        """ä¼°ç®—æ–‡æœ¬çš„tokenæ•°é‡ï¼ˆç®€å•ä¼°ç®—ï¼‰"""
        if not text:
            return 0
        
        import re
        # ç®€å•çš„tokenä¼°ç®—ï¼šä¸­æ–‡å­—ç¬¦çº¦1ä¸ªtokenï¼Œè‹±æ–‡å•è¯çº¦1ä¸ªtoken
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text))
        # å»æ‰ä¸­æ–‡å­—ç¬¦åè®¡ç®—è‹±æ–‡å•è¯
        text_without_chinese = re.sub(r'[\u4e00-\u9fff]', '', text)
        english_words = len(text_without_chinese.split())
        
        return chinese_chars + english_words
    
    def get_last_routing_result(self):
        """è·å–æœ€åä¸€æ¬¡è·¯ç”±ç»“æœ"""
        return getattr(self, 'last_routing_result', None)
    
    def get_processed_prompt(self):
        """è·å–HOOKå¤„ç†åçš„æç¤ºè¯"""
        return getattr(self, 'processed_prompt', None)
    
    def get_processed_headers(self):
        """è·å–HOOKå¤„ç†åå‘é€ç»™å¤§æ¨¡å‹çš„è¯·æ±‚å¤´"""
        return getattr(self, 'processed_headers', None)
    
    def get_model_raw_headers(self):
        """è·å–å¤§æ¨¡å‹è¿”å›çš„åŸå§‹å“åº”å¤´"""
        return getattr(self, 'model_raw_headers', None)
    
    def get_model_raw_response(self):
        """è·å–å¤§æ¨¡å‹è¿”å›çš„åŸå§‹å“åº”ä½“(HOOKå¤„ç†å‰)"""
        return getattr(self, 'model_raw_response', None)
    
    def get_token_usage(self):
        """è·å–Tokenä½¿ç”¨é‡"""
        if hasattr(self, 'streaming_converter') and self.streaming_converter:
            return {
                "input_tokens": self.streaming_converter.total_input_tokens,
                "output_tokens": self.streaming_converter.total_output_tokens,
                "total_tokens": self.streaming_converter.total_input_tokens + self.streaming_converter.total_output_tokens
            }
        return {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0}

# å…¨å±€æœåŠ¡å®ä¾‹
multi_platform_service = MultiPlatformService()